# 179880 - study with multiple interventions - impacts for one intervention were recorded with the wrong units
# Re-recording these impacts to best align with the rest of the data
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Khuntil")] <- -0.51
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Magharvada")] <- 0.26
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Bhalot")] <- 0.07
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Bharu")] <- 0.06
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Sitara")] <- 0.06
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Purkhawas")] <- 0.06
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Chomakot")] <- 0.02
# Fixing other impacts from this study - perrenials had lower area compared to annuals
dataset_clean$study_subject_value[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Khuntil")] <- 93
dataset_clean$study_subject_value[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Purkhawas")] <- 15
dataset_clean$study_subject_value[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Bhalot")] <- 7.5
# # Same study, wrong impact collected initially
# dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Bhalot")] <- 0.36
# 205460 - study used commas rather than decimals.
dataset_clean$emissions_reduction_impact[dataset_clean$unique_id == "205460"] <- 1269000 # (2.3848-1.1158)*1000000
# 110797 - results actually for a representative field and its unclear how many km or ha in IL are dedicated to bioenergy crops
dataset_clean$subnational_wide[dataset_clean$unique_id == "110797"] <- "No"
dataset_clean$synthesis_group[dataset_clean$unique_id == "110797"] <- "sg_3"
# 137761 - units are tCO2/month so time frame should be 1/12
dataset_clean$impact_time_frame[dataset_clean$unique_id == "137761"] <- 1/12
# Exclude Scotland and Wales as subnationals - REMOVES 2 ARTICLES
dataset_clean <- dataset_clean %>%
filter(name != "Wales") %>%
filter(name != "Scotland") %>% # Remove Wales and Scotland as subnationals
# Two observations missing synthesis group, these are subnational-wide
dataset_clean <- dataset_clean %>%
mutate(synthesis_group = ifelse(is.na(synthesis_group), "sg_1", synthesis_group))
dataset_clean <- dataset_nocontext %>%
filter(!is.na(population)) %>% # drop out the observations without population
bind_rows(dataset_fillcontext) %>% # bring in the observations with manually
bind_rows(dataset_context) %>%
select(-c("X", "title","wiki_iso", "categories", "wiki_type", "area", "area_units", "region_hierarchy", "elevation",
"gdp", "gdp_units", "gdp_year", "area_year", "initiatives_committed", "num_commit", "state", "region"))
# Merge in region from country_dict
region_map <- country_dict %>%
distinct(iso, region)
dataset_clean <- dataset_clean %>%
left_join(region_map, by="iso")
##### 6. Final Changes and Export #####
# Rename/Combine some Tier 2 Categories
dataset_clean <- dataset_clean %>%
mutate(mitigation_stratcat_t2 = case_when((mitigation_strategy_cat == "Energy efficiency measures" & sector=="Electricity and heat") ~ "Energy system efficiencies",
(mitigation_strategy_cat == "Smart grid, Smart meters, intelligent controls, thermostats, building information and monitoring systems" & sector=="Electricity and heat") ~ "Energy system efficiencies",
(mitigation_strategy_cat == "Waste heat recovery" & sector=="Electricity and heat") ~ "Energy system efficiencies",
mitigation_strategy_cat == "Waste segregation" ~ "Waste system management",
mitigation_strategy_cat == "Water management system efficiencies" ~ "Water and wastewater system efficiencies",
(mitigation_strategy_cat == "Demand side energy management, optimization, peak shifting/ shaving" & sector=="Waste") ~ "Water and wastewater system efficiencies",
mitigation_strategy_cat == "Transition to low emissions fuels" ~ "Fuel switching and fuel efficiency",
(mitigation_strategy_cat == "Smart grid, Smart meters, intelligent controls, thermostats, building information and monitoring systems" & sector== "Buildings") ~ "Smart meters, intelligent controls, thermostats, building information and monitoring systems",
(mitigation_strategy_cat == "Smart grid, Smart meters, intelligent controls, thermostats, building information and monitoring systems" & sector== "Electricity and heat") ~ "Energy system efficiencies",
TRUE ~ mitigation_strategy_cat)) %>%
mutate(mitigation_stratcat_t2 = case_when(intervention %in% c("Use lights 10 minutes less each day", "temperature setbacks during lighter building load conditions", "Thermostat temperature setbacks") ~ "Demand side energy management, optimization, peak shifting/ shaving",
TRUE ~ mitigation_stratcat_t2))
# Define Tier 1 Categories:
dataset_clean <- dataset_clean %>%
mutate(mitigation_stratcat_t1 = case_when(mitigation_stratcat_t2 == "Agricultural land management" ~ "Agriculture and farm practices",
mitigation_stratcat_t2 == "Biomass, biodiesel, biomass gasification, ethanol production" ~ "Agriculture and farm practices",
mitigation_stratcat_t2 == "Urban agriculture" ~ "Agriculture and farm practices",
mitigation_stratcat_t2 == "Master planning, integrated planning, urban form, design, planning" ~ "Land use and development",
mitigation_stratcat_t2 == "Transit oriented development" ~ "Land use and development",
mitigation_stratcat_t2 == "Afforestation, greening" ~ "Land use and development",
mitigation_stratcat_t2 == "Cool roof/facade, roof garden, green roof" ~ "Building construction and improvement",
(mitigation_stratcat_t2 == "Energy efficiency measures" & sector == "Buildings") ~ "Building construction and improvement",
mitigation_stratcat_t2 == "Green building, passive solar building design, net zero emission building, carbon neutral building" ~ "Building construction and improvement",
mitigation_stratcat_t2 == "Retrofitting" ~ "Building construction and improvement",
mitigation_stratcat_t2 == "Demand side energy management, optimization, peak shifting/ shaving" ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Cogeneration, combined heat and power, tri-generation, combined heat, cooling, and power" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "District heating/cooling, expansion" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Energy storage, batteries" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Other renewable energy" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Smart meters, intelligent controls, thermostats, building information and monitoring systems" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Solar PV energy" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Solar PV thermal, Solar PV cooling, Solar trigeneration CPVT" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Fuel switching and fuel efficiency" & sector == "Buildings") ~ "Building energy and heat systems",
(mitigation_stratcat_t2 == "Geothermal heat pumps, ground source heat pumps" & sector == "Buildings") ~ "Building energy and heat systems",
mitigation_stratcat_t2 == "Carbon capture and storage, carbon capture and sequestration" ~ "Clean energy generation",
mitigation_stratcat_t2 == "Low-carbon energy generation" ~ "Clean energy generation",
(mitigation_stratcat_t2 == "Other renewable energy" & sector == "Electricity and heat") ~ "Clean energy generation",
(mitigation_stratcat_t2 == "Solar PV energy" & sector == "Electricity and heat") ~ "Clean energy generation",
(mitigation_stratcat_t2 == "Wind energy" & sector == "Electricity and heat") ~ "Clean energy generation",
(mitigation_stratcat_t2 == "Cogeneration, combined heat and power, tri-generation, combined heat, cooling, and power" & sector == "Electricity and heat") ~ "Energy system operations",
(mitigation_stratcat_t2 == "District heating/cooling, expansion" & sector == "Electricity and heat") ~ "Energy system operations",
(mitigation_stratcat_t2 == "Energy system efficiencies" & sector == "Electricity and heat") ~ "Energy system operations",
(mitigation_stratcat_t2 == "Energy storage, batteries" & sector == "Electricity and heat") ~ "Energy system operations",
mitigation_stratcat_t2 == "Circular economy, industrial symbiosis, use of recycled materials" ~ "Circular economy, industrial symbiosis, use of recycled materials",
mitigation_stratcat_t2 == "Emissions trading, cap-and-trade, carbon tax" ~ "Emissions trading, cap-and-trade, carbon tax",
mitigation_stratcat_t2 == "Non-motorized transport" ~ "Alternative transportation modes",
mitigation_stratcat_t2 == "Public transport" ~ "Alternative transportation modes",
mitigation_stratcat_t2 == "Ride sharing, carpooling" ~ "Alternative transportation modes",
mitigation_stratcat_t2 == "Electric Mobility (EV, HEV)" & sector == "Transport" ~ "Clean vehicle transportation",
(mitigation_stratcat_t2 == "Fuel switching and fuel efficiency" & sector == "Transport") ~ "Clean vehicle transportation",
mitigation_stratcat_t2 == "Intelligent transportation system" ~ "Transportation system management",
mitigation_stratcat_t2 == "Travel demand management" ~ "Transportation system management",
mitigation_stratcat_t2 == "Composting and biological treatment of waste" ~ "Waste and water treatment practices",
mitigation_stratcat_t2 == "Waste system management" ~ "Waste and water treatment practices",
mitigation_stratcat_t2 == "Waste to energy, energy from waste" ~ "Waste and water treatment practices",
mitigation_stratcat_t2 == "Water and wastewater system efficiencies" ~ "Waste and water treatment practices",
(mitigation_stratcat_t2 == "Cogeneration, combined heat and power, tri-generation, combined heat, cooling, and power" & sector == "Industry") ~ "Industrial facility improvements",
(mitigation_stratcat_t2 == "Electric Mobility (EV, HEV)" & sector == "Industry") ~ "Industrial facility improvements",
(mitigation_stratcat_t2 == "Solar PV energy" & sector == "Industry") ~ "Industrial facility improvements",
(mitigation_stratcat_t2 == "Fuel switching and fuel efficiency" & sector == "Industry") ~ "Industrial facility improvements",
(mitigation_stratcat_t2 == "Waste heat recovery" & sector == "Industry") ~ "Industrial facility improvements"))
# Remove observations that could not be categorized (3 from one study)
dataset_clean <- dataset_clean %>%
filter(!is.na(mitigation_strategy_cat))
# Replace land use studies in other sectors to "Agriculture and forestry"
# dataset_clean <- dataset_clean %>%
#   mutate(sector = ifelse(mitigation_stratcat_t1 == "Land use and development","Agriculture and forestry", sector))
# FINAL MANUAL CLEANING
# REMOVE ARTICLE: 39559 not subnational wide, uses sample of 50 households
dataset_clean$subnational_wide[dataset_clean$unique_id == "39559"] <- "No"
dataset_clean$synthesis_group[dataset_clean$unique_id == "39559"] <- "sg_2"
# Articles where population was pulled incorrectly
# 346836 - population of Piedade Brazil incorrectly set to 2. Update from Wikipedia
dataset_clean$population[(dataset_clean$unique_id == "346836" & dataset_clean$name == "Piedade")] <- 55542
# 226608
dataset_clean$population[(dataset_clean$unique_id == "226608" & dataset_clean$name == "Khulna")] <- 949229
dataset_clean$population[(dataset_clean$unique_id == "226608" & dataset_clean$name == "Rangpur")] <- 1031388
# 128045
dataset_clean$population[(dataset_clean$unique_id == "128045" & dataset_clean$name == "Chongqing")] <- 32054159
# 128249
dataset_clean$population[(dataset_clean$unique_id == "128249" & dataset_clean$name == "Tehran")] <- 9039000
# 99374
dataset_clean$population[(dataset_clean$unique_id == "99374" & dataset_clean$name == "Adelaide")] <- 1387290
# 100129
dataset_clean$population[(dataset_clean$unique_id == "100129" & dataset_clean$name == "Sheffield")] <-556500
# 93147
dataset_clean$impact_time_frame[(dataset_clean$unique_id == "93147")] <-"2020-2050"
# 206292 - incorrectly collected results for a national LCFS scenario rather than the CA scenario
dataset_clean <- dataset_clean %>%
filter(!(unique_id == "206292" & emissions_reduction_impact == 394)) # only need one impact
dataset_clean$emissions_reduction_impact[dataset_clean$unique_id == "206292"] <- 26.9
dataset_clean$impact_time_period[dataset_clean$unique_id == "206292"] <- 2020
dataset_clean$intervention[dataset_clean$unique_id == "206292"] <- "Low carbon fuel policy, assuming optimistic technology adoption rates where lifecycle GHG emissions from transportation fuel use are reduced by 10% in 2020"
# 195004
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "195004" & dataset_clean$intervention == "Increased wind capacity (no nuclear, current transmission capacity, $100 carbon tax)")] <- 3.94
# 128045
dataset_clean$population[(dataset_clean$unique_id == "128045" & dataset_clean$name=="Guangdong")] <- 126012510
dataset_clean$subnational_context[(dataset_clean$unique_id == "128045" & dataset_clean$name=="Guangdong")] <- "Regional Government"
dataset_clean$subnational_context[(dataset_clean$unique_id == "128045" & dataset_clean$name=="Hubei")] <- "Regional Government"
# 179880 - study with multiple interventions - impacts for one intervention were recorded with the wrong units
# Re-recording these impacts to best align with the rest of the data
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Khuntil")] <- -0.51
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Magharvada")] <- 0.26
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Bhalot")] <- 0.07
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Bharu")] <- 0.06
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Sitara")] <- 0.06
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Purkhawas")] <- 0.06
dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Improved practices in fertilizer application for annuals and perennials" & dataset_clean$name=="Chomakot")] <- 0.02
# Fixing other impacts from this study - perrenials had lower area compared to annuals
dataset_clean$study_subject_value[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Khuntil")] <- 93
dataset_clean$study_subject_value[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Purkhawas")] <- 15
dataset_clean$study_subject_value[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Bhalot")] <- 7.5
# # Same study, wrong impact collected initially
# dataset_clean$emissions_reduction_impact[(dataset_clean$unique_id == "179880" & dataset_clean$intervention == "Adoption of climate-resilient practices for perennial crops" & dataset_clean$name=="Bhalot")] <- 0.36
# 205460 - study used commas rather than decimals.
dataset_clean$emissions_reduction_impact[dataset_clean$unique_id == "205460"] <- 1269000 # (2.3848-1.1158)*1000000
# 110797 - results actually for a representative field and its unclear how many km or ha in IL are dedicated to bioenergy crops
dataset_clean$subnational_wide[dataset_clean$unique_id == "110797"] <- "No"
dataset_clean$synthesis_group[dataset_clean$unique_id == "110797"] <- "sg_3"
# 137761 - units are tCO2/month so time frame should be 1/12
dataset_clean$impact_time_frame[dataset_clean$unique_id == "137761"] <- 1/12
# Exclude Scotland and Wales as subnationals - REMOVES 2 ARTICLES
dataset_clean <- dataset_clean %>%
filter(name != "Wales") %>%
filter(name != "Scotland") # Remove Wales and Scotland as subnationals
# Two observations missing synthesis group, these are subnational-wide
dataset_clean <- dataset_clean %>%
mutate(synthesis_group = ifelse(is.na(synthesis_group), "sg_1", synthesis_group))
## Clear R Environment ----
rm(list = ls())
R.version
## Libraries ----
library(tidyverse)
library(readxl)
library(tidytext)
library(igraph)
library(ggraph)
# 1. Read in both sets of data
og <- read_excel("../Data/Analysis/03_First_Round_Data1.xlsx",
sheet = "03_First_Round_Data1")
og2 <- read_excel("../Data/Analysis/03_First_Round_Data2.xlsx",
sheet = "03_First_Round_Data2")
df <- bind_rows(og, og2)
rm(og, og2)
gc()
View(df)
## De-duplicate the datasets by removing all special characters and spaces ----
df <- df %>% filter(!duplicated(gsub("[^a-zA-Z0-9]", "", tolower(df$Title))))
## Eliminate those that are older than 2010 ----
df <- df %>%
filter(as.numeric(Year) >= 2010)
## check for those with actors names only after the copyright symbol ----
df <- df %>%
separate(Abstract, into = c("pre_copyright", "post_copyright"),
sep = "©", remove = F) %>%
mutate(entity.term = coalesce(City.Term, Region.Term),
flag = str_detect(post_copyright, fixed(entity.term)) & !str_detect(pre_copyright, fixed(entity.term))) %>%
filter(!flag|is.na(flag)) # remove those that have the entity term only post copyright sign
## Filter out articles that contain the stopwords in the abstract and title ----
stopwords <- c(
"air pollut", "PM2.5", "meta-analysis", "literature review",
"the proceedings contain(s){0,1}", "isotope", "meteorolog",
"corrigendum", "erratum", "Correction", "bacteria", "oxid",
"health"
)
df <- df %>%
filter(!(str_detect(Abstract, regex(paste0(stopwords, collapse = "|"), ignore_case = T))|
str_detect(Title, regex(paste0(stopwords, collapse ="|"), ignore_case = T))))
View(df)
# Filter out articles flagged on generic subnational terrms
general_entity <- c(
"University", "Park", "LES", "^West$", "^Western$",
"^East$", "^Eastern$", "^North$", "^Northern$", "^South$",
"^Southern$"
)
df <- df %>%
filter(!str_detect(df$entity.term, paste0(general_entity, collapse = "|")))
View(df)
# Preserve current sample of included articles for validation
sample <- df[df$Include=="Yes" & !is.na(df$Include),]
## Generate bigrams ----
bigram <- df %>%
unnest_tokens(bigram, Abstract, token = "ngrams", n = 2)
bigrams_count <- bigram %>%
group_by(Title) %>%
count(bigram, sort = T)
## Remove those with stop words and weird combinations ----
bigrams_sep <- bigrams_count %>%
separate(bigram, c("bigram_1", "bigram_2"), sep = " ")
View(df)
## Unite the bigrams and see which bigrams appeared the most in documents ----
bigrams_combined <- bigrams_sep %>%
unite("bigram", bigram_1, bigram_2, sep = " ") %>%
group_by(bigram) %>%
count(bigram, sort = T)
topical_words <- c("climate", "greenhouse", "gas", "ghg", "mitigat",
"carbon", "CO2", "CO 2", "CH4", "CH 4", "methane",
"N20", "N 2 O", "N2 O", "energy", "electricity",
"fossil", "fuel")
positive_words <- c(
"emission(s){0,1}", "avoid", "effective", "abate",
"reduc", "quanti", "efficie", "impact"
)
bigrams_keep <- bigrams_combined %>%
filter(str_detect(bigram, regex(paste0(topical_words, collapse = "|"),
ignore_case = T))) %>%
filter(str_detect(bigram, regex(paste0(positive_words, collapse = "|"),
ignore_case = T)))
View(df)
rm(bigram, bigrams_count, bigrams_sep)
gc()
## Now use the bigrams to filter the main abstracts ----
final <- df %>%
filter(str_detect(Abstract, regex(paste0(bigrams_keep$bigram, collapse = "|"),
ignore_case = T)))
View(final)
View(df)
View(bigrams_keep)
rm(sample, bigrams_combined, bigrams_keep)
# 1. Read in both sets of data
og <- read_excel("../Data/Analysis/03_First_Round_Data1.xlsx",
sheet = "03_First_Round_Data1")
og2 <- read_excel("../Data/Analysis/03_First_Round_Data2.xlsx",
sheet = "03_First_Round_Data2")
df <- bind_rows(og, og2)
rm(og, og2)
View(df)
## De-duplicate the datasets by removing all special characters and spaces ----
df <- df %>% filter(!duplicated(gsub("[^a-zA-Z0-9]", "", tolower(df$Title))))
View(df)
## Eliminate those that are older than 2010 ----
df <- df %>%
filter(as.numeric(Year) >= 2010)
## check for those with actors names only after the copyright symbol ----
df <- df %>%
separate(Abstract, into = c("pre_copyright", "post_copyright"),
sep = "©", remove = F) %>%
mutate(entity.term = coalesce(City.Term, Region.Term),
flag = str_detect(post_copyright, fixed(entity.term)) & !str_detect(pre_copyright, fixed(entity.term))) %>%
filter(!flag|is.na(flag)) # remove those that have the entity term only post copyright sign
View(df)
# 1. Read in both sets of data
og <- read_excel("../Data/Analysis/03_First_Round_Data1.xlsx",
sheet = "03_First_Round_Data1")
og2 <- read_excel("../Data/Analysis/03_First_Round_Data2.xlsx",
sheet = "03_First_Round_Data2")
df <- bind_rows(og, og2)
rm(og, og2)
gc()
View(df)
View(df)
## De-duplicate the datasets by removing all special characters and spaces ----
df <- df %>% filter(!duplicated(gsub("[^a-zA-Z0-9]", "", tolower(df$Title))))
View(df)
## Eliminate those that are older than 2010 ----
df <- df %>%
filter(as.numeric(Year) >= 2010)
## check for those with actors names only after the copyright symbol ----
df <- df %>%
separate(Abstract, into = c("pre_copyright", "post_copyright"),
sep = "©", remove = F) %>%
mutate(entity.term = coalesce(City.Term, Region.Term),
flag = str_detect(post_copyright, fixed(entity.term)) & !str_detect(pre_copyright, fixed(entity.term))) %>%
filter(!flag|is.na(flag)) # remove those that have the entity term only post copyright sign
View(df)
## Filter out articles that contain the stopwords in the abstract and title ----
stopwords <- c(
"air pollut", "PM2.5", "meta-analysis", "literature review",
"the proceedings contain(s){0,1}", "isotope", "meteorolog",
"corrigendum", "erratum", "Correction", "bacteria", "oxid",
"health"
)
df <- df %>%
filter(!(str_detect(Abstract, regex(paste0(stopwords, collapse = "|"), ignore_case = T))|
str_detect(Title, regex(paste0(stopwords, collapse ="|"), ignore_case = T))))
View(df)
# Filter out articles flagged on generic subnational terrms
general_entity <- c(
"University", "Park", "LES", "^West$", "^Western$",
"^East$", "^Eastern$", "^North$", "^Northern$", "^South$",
"^Southern$"
)
df <- df %>%
filter(!str_detect(df$entity.term, paste0(general_entity, collapse = "|")))
View(df)
View(final)
rm(list = ls())
library(tidyverse)
library(readxl)
library(writexl)
library(openxlsx)
library(tidytext)
library(igraph)
library(ggraph)
# library(ClimActor)
# install.packages("pdftools")
library("pdftools")
library(stringr)
# install.packages("tokenizers")
library(tokenizers)
library(ClimActor)
library(rworldmap)
library(maps)
library(ggrepel)
library(ggpubr)
library(scales)
library(boot)
library(mosaic)
library(rms)
library(metafor)
library(robumeta)
##### Bring in Final Actions Dataset - START HERE! #####
gov_actions_orig <- read_excel("../Data/Manual_Extraction/Gov_Actions_Final.xlsx")
gov_actions <- gov_actions_orig %>%
filter(policy_status != "Under Consideration") %>% # Going to focus on planned and implemented actions
mutate(sector = case_when(mitigation_stratcat_t1 %in% c("Circular economy, industrial symbiosis, use of recycled materials",
"Emissions trading, cap-and-trade, carbon tax",
"Land use and development") ~ "Cross-Sectoral",
TRUE ~ sector))
# Fix some lat lons
gov_actions$lat[gov_actions$name == "California"] <- 35.458611
gov_actions$lng[gov_actions$name == "California"] <- -119.355278
gov_actions$lat[gov_actions$name == "Hubei"] <- 30.593400
gov_actions$lng[gov_actions$name == "Hubei"] <- 114.304600
# How many studies?
length(unique(gov_actions$unique_id))
# How many actors?
actors <- gov_actions %>%
distinct(unique_id, name, country, iso, global_region, subnational_context, lat, lng) %>%
count(name, country, iso, global_region, subnational_context, lat, lng)
# What kind of strategies? - essentially unique by actor, strategy, and study/policy note
strategies <- gov_actions %>%
count(name, country, iso, global_region, mitigation_stratcat_t1, mitigation_stratcat_t2, subnational_context, sector, lat, lng, policy_note, broader_goal)
View(strategies)
# Table of Policy Instruments
instrument_sum <- gov_actions %>%
count(sector, policy_instrument)
View(instrument_sum)
# Demand vs. Supply vs. Mixed?
supply_demand <- gov_actions %>%
count(supply_or_demand) %>%
mutate(total = sum(n),
pct = n/total)
##### 4. Stats #####
# Most Common Country?
country_sum <- strategies %>%
count(iso)
# Most Common Region?
region_sum <- strategies %>%
count(global_region)
View(supply_demand)
# Policy "smart packaging"
table(gov_actions$broader_goal)
packaging_sum <- gov_actions %>%
count(broader_goal, sector) %>%
pivot_wider(id_cols=sector, names_from=broader_goal, values_from=n) %>%
replace(is.na(.), 0) %>%
mutate(total = Yes + No,
pct = Yes/total)
View(packaging_sum)
table(gov_actions$subnational_context)
table(strategies$subnational_context)
table(actors$subnational_context)
11/50
39/50
names(gov_actions)
# Most Common Instrument Types by Actor Type
actor_type_instruments <- gov_actions %>%
count(subnational_context, policy_instrument)
View(actor_type_instruments)
# Most Common Instrument Types by Actor Type
actor_type_instruments <- gov_actions %>%
count(subnational_context, policy_instrument) %>%
group_by(subnational_context) %>%
mutate(total = sum(n)) %>%
ungroup() %>%
mutate(pct = n/total)
View(actor_type_instruments)
table(gov_actions$subnational_context)
104/134
table(gov_actions$global_region)
19/30
43+26
69/104
rm(list = ls())
library(tidyverse)
library(readxl)
library(writexl)
library(openxlsx)
library(tidytext)
library(igraph)
library(ggraph)
# library(ClimActor)
# install.packages("pdftools") # We need this to extract text from the PDFs!
library("pdftools")
library(stringr)
# install.packages("tokenizers")
library(tokenizers)
library(ClimActor)
library(rworldmap)
library(maps)
library(ggrepel)
library(ggpubr)
library(scales)
##### 1. Bring in Data #####
tracker <- read_excel("../Data/Manual_Extraction/Data_Screening_and_Extraction_Final.xlsx",
sheet = "Tracker", col_names = TRUE, range="A1:O290") # skip = 1
drop_downs <- read_excel("../Data/Manual_Extraction/Data_Screening_and_Extraction_Final.xlsx",
sheet = "Drop Downs", col_names = TRUE)
extr_other <- read_excel("../Data/Manual_Extraction/Data_Screening_and_Extraction_Final.xlsx",
sheet = "Dataset", col_names = TRUE, skip = 1)
extr_workshop <- read.csv("../Data/Manual_Extraction/Workshop_Survey_Responses_Clean_Final.csv",
header=T, as.is=T, stringsAsFactors = FALSE, encoding = "UTF-8", na.strings=c("", "NA"))
View(extr_other)
View(extr_workshop)
View(drop_downs)
##### 2. Align and Combine #####
drop_downs <- drop_downs %>%
select(-c("...1", "...7", "Intervention Type/Policy Instrument")) %>%
rename("mitigation_strategy_cat" = "Mitigation Strategy",
"subnat_type" = "Subnational Context",
"sector" = "Sector",
"impact_type" = "Type of Impact")
View(drop_downs)
tracker <- tracker %>%
mutate(across(everything(), as.character)) %>%
mutate(workshop = ifelse(extracted == "Workshop", "Yes", "No")) %>%
mutate(extracted = case_when(unique_id == "106351" ~ "No",
unique_id == "113145" ~ "No",
unique_id == "90709" ~ "No",
unique_id == "95478" ~ "No",
TRUE ~ extracted)) %>%
mutate(extracted = replace(extracted, extracted == "Workshop", "Yes"))
View(extr_workshop)
extr_workshop <- extr_workshop %>%
rename(extractor_name = name) %>%
select(-c("observation", "group_id", "obs_num", "article_link", "more_impacts")) %>%
mutate(across(everything(), as.character))
extr_other <- extr_other %>%
select(-c("screener_name", "set_id", "year", "title", "authors", "include", "city.term", "region.term", "doi", "...42")) %>%
mutate(across(everything(), as.character)) %>%
filter(!unique_id %in% extr_workshop$unique_id)
# Combine extracted data
dataset <- extr_other %>%
bind_rows(extr_workshop)
# Remove articles that were screened out during the extraction
to_remove <- unique(tracker$unique_id[tracker$extracted=="No"])
dataset <- dataset %>%
filter(!unique_id %in% to_remove)
rm(list = ls())
library(tidyverse)
library(readxl)
library(writexl)
library(tidytext)
library(igraph)
library(ggraph)
# library(ClimActor)
# install.packages("pdftools") # We need this to extract text from the PDFs!
library("pdftools")
library(stringr)
# install.packages("tokenizers")
library(tokenizers)
articles_sample <- list.files("C:/Data_Extraction_Sample_50",
pattern="\\.pdf$") # Location of files on local computer
# Bring in article text
articles_text1 <- lapply(articles_sample, pdftools::pdf_text)
articles_sample
# Bring in article text
articles_text1 <- lapply(articles_sample, pdftools::pdf_text)
